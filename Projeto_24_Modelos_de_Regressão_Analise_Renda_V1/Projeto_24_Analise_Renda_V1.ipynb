{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M_HTczGnhTa"
      },
      "source": [
        "# EBAC - Regressão II - regressão múltipla\n",
        "\n",
        "## Tarefa I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni8WoBSpnhTq"
      },
      "source": [
        "#### Previsão de renda II\n",
        "\n",
        "Vamos continuar trabalhando com a base 'previsao_de_renda.csv', que é a base do seu próximo projeto. Vamos usar os recursos que vimos até aqui nesta base.\n",
        "\n",
        "|variavel|descrição|\n",
        "|-|-|\n",
        "|data_ref                | Data de referência de coleta das variáveis |\n",
        "|index                   | Código de identificação do cliente|\n",
        "|sexo                    | Sexo do cliente|\n",
        "|posse_de_veiculo        | Indica se o cliente possui veículo|\n",
        "|posse_de_imovel         | Indica se o cliente possui imóvel|\n",
        "|qtd_filhos              | Quantidade de filhos do cliente|\n",
        "|tipo_renda              | Tipo de renda do cliente|\n",
        "|educacao                | Grau de instrução do cliente|\n",
        "|estado_civil            | Estado civil do cliente|\n",
        "|tipo_residencia         | Tipo de residência do cliente (própria, alugada etc)|\n",
        "|idade                   | Idade do cliente|\n",
        "|tempo_emprego           | Tempo no emprego atual|\n",
        "|qt_pessoas_residencia   | Quantidade de pessoas que moram na residência|\n",
        "|renda                   | Renda em reais|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1l_RQssEnhTs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wiB4DueknhTt"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('previsao_de_renda.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_4EB7mdnhTv",
        "outputId": "c3282f1f-e33b-437d-d95e-a4858a835fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15000 entries, 0 to 14999\n",
            "Data columns (total 15 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   Unnamed: 0             15000 non-null  int64  \n",
            " 1   data_ref               15000 non-null  object \n",
            " 2   id_cliente             15000 non-null  int64  \n",
            " 3   sexo                   15000 non-null  object \n",
            " 4   posse_de_veiculo       15000 non-null  bool   \n",
            " 5   posse_de_imovel        15000 non-null  bool   \n",
            " 6   qtd_filhos             15000 non-null  int64  \n",
            " 7   tipo_renda             15000 non-null  object \n",
            " 8   educacao               15000 non-null  object \n",
            " 9   estado_civil           15000 non-null  object \n",
            " 10  tipo_residencia        15000 non-null  object \n",
            " 11  idade                  15000 non-null  int64  \n",
            " 12  tempo_emprego          12427 non-null  float64\n",
            " 13  qt_pessoas_residencia  15000 non-null  float64\n",
            " 14  renda                  15000 non-null  float64\n",
            "dtypes: bool(2), float64(3), int64(4), object(6)\n",
            "memory usage: 1.5+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYy1jiienhTw"
      },
      "source": [
        "1. Separe a base em treinamento e teste (25% para teste, 75% para treinamento).\n",
        "2. Rode uma regularização *ridge* com alpha = [0, 0.001, 0.005, 0.01, 0.05, 0.1] e avalie o $R^2$ na base de testes. Qual o melhor modelo?\n",
        "3. Faça o mesmo que no passo 2, com uma regressão *LASSO*. Qual método chega a um melhor resultado?\n",
        "4. Rode um modelo *stepwise*. Avalie o $R^2$ na vase de testes. Qual o melhor resultado?\n",
        "5. Compare os parâmetros e avalie eventuais diferenças. Qual modelo você acha o melhor de todos?\n",
        "6. Partindo dos modelos que você ajustou, tente melhorar o $R^2$ na base de testes. Use a criatividade, veja se consegue inserir alguma transformação ou combinação de variáveis.\n",
        "7. Ajuste uma árvore de regressão e veja se consegue um $R^2$ melhor com ela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jytjj42nhTx",
        "outputId": "a5d8bf35-7250-432c-85ae-1018f74e9b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'data_ref', 'id_cliente', 'sexo', 'posse_de_veiculo',\n",
            "       'posse_de_imovel', 'qtd_filhos', 'tipo_renda', 'educacao',\n",
            "       'estado_civil', 'tipo_residencia', 'idade', 'tempo_emprego',\n",
            "       'qt_pessoas_residencia', 'renda'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# 1 - Separe a base em treinamento e teste (25% para teste, 75% para treinamento).\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Carregar a base\n",
        "df = pd.read_csv('previsao_de_renda.csv')\n",
        "\n",
        "# Verificar quais colunas estão disponíveis\n",
        "print(df.columns)\n",
        "\n",
        "# Remover apenas as colunas que existem no DataFrame\n",
        "colunas_para_remover = ['renda', 'data_ref', 'index']\n",
        "colunas_existentes = [col for col in colunas_para_remover if col in df.columns]\n",
        "\n",
        "X = df.drop(colunas_existentes, axis=1)\n",
        "y = df['renda']\n",
        "\n",
        "# Transformar variáveis categóricas em dummies\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Dividir em treino e teste (75% treino, 25% teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Rode uma regularização ridge com alpha = [0, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "#e avalie o R2 na base de testes. Qual o melhor modelo?\n",
        "\n",
        "# Importações\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Carregar a base\n",
        "df = pd.read_csv('previsao_de_renda.csv')\n",
        "\n",
        "# Remover colunas irrelevantes se existirem\n",
        "colunas_para_remover = ['renda', 'data_ref', 'index']\n",
        "colunas_existentes = [col for col in colunas_para_remover if col in df.columns]\n",
        "X = df.drop(colunas_existentes, axis=1)\n",
        "y = df['renda']\n",
        "\n",
        "# Transformar variáveis categóricas em dummies\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Dividir os dados em treino e teste (75% treino, 25% teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Imputar valores ausentes com a média\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Lista de alphas e dicionário para armazenar os resultados\n",
        "alphas = [0, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "ridge_results = {}\n",
        "\n",
        "# Loop para treinar o modelo com diferentes alphas\n",
        "for alpha in alphas:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_imputed, y_train)\n",
        "    y_pred = ridge.predict(X_test_imputed)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    ridge_results[alpha] = r2\n",
        "\n",
        "# Exibir os resultados\n",
        "print(\"Resultados Ridge:\", ridge_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf7q8STCqmj6",
        "outputId": "f9e56f05-3e37-49e8-c2de-9054b9fb4e2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados Ridge: {0: 0.26882468105695445, 0.001: 0.26882468839759666, 0.005: 0.2688247177284102, 0.01: 0.26882475432063024, 0.05: 0.2688250442364063, 0.1: 0.26882539971808883}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Melhor Modelo - Regularização Ridge\n",
        "\n",
        "Com base nos resultados fornecidos, o melhor modelo é aquele que apresenta o maior valor de \\( R^2 \\).\n",
        "\n",
        "### Resultados por Alpha:\n",
        "- **alpha = 0:** 0.2688  \n",
        "- **alpha = 0.001:** 0.2688  \n",
        "- **alpha = 0.005:** 0.2688  \n",
        "- **alpha = 0.01:** 0.2688  \n",
        "- **alpha = 0.05:** 0.2688  \n",
        "- **alpha = 0.1:** 0.2688  \n",
        "\n",
        "### Melhor Modelo  \n",
        "O melhor modelo é com **alpha = 0.1**, pois apresenta o maior valor de \\( R^2 \\) (**0.26883**). Esse modelo oferece o melhor ajuste na base de testes entre os avaliados.\n",
        "\n",
        "Embora a diferença entre os valores seja pequena, como o objetivo é maximizar o \\( R^2 \\), **alpha = 0.1** é a escolha ideal.\n"
      ],
      "metadata": {
        "id": "bsy-hnpDtHBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 - Faça o mesmo que no passo 2, com uma regressão LASSO.\n",
        "# Qual método chega a um melhor resultado?\n",
        "\n",
        "# Importações\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Carregar a base de dados\n",
        "df = pd.read_csv('previsao_de_renda.csv')\n",
        "\n",
        "# Remover colunas irrelevantes, se existirem\n",
        "colunas_para_remover = ['renda', 'data_ref', 'index']\n",
        "colunas_existentes = [col for col in colunas_para_remover if col in df.columns]\n",
        "X = df.drop(colunas_existentes, axis=1)\n",
        "y = df['renda']\n",
        "\n",
        "# Transformar variáveis categóricas em dummies\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Dividir os dados em treino e teste (75% treino, 25% teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Imputar valores ausentes com a média\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Lista de alphas e dicionário para armazenar os resultados\n",
        "alphas = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "lasso_results = {}\n",
        "\n",
        "# Loop para treinar o modelo com diferentes alphas\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    lasso.fit(X_train_imputed, y_train)\n",
        "    y_pred = lasso.predict(X_test_imputed)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    lasso_results[alpha] = r2\n",
        "\n",
        "# Exibir os resultados\n",
        "print(\"Resultados LASSO:\", lasso_results)\n",
        "\n",
        "# Identificar o melhor modelo\n",
        "melhor_alpha = max(lasso_results, key=lasso_results.get)\n",
        "melhor_r2 = lasso_results[melhor_alpha]\n",
        "\n",
        "# Exibir o melhor modelo\n",
        "print(f\"\\nMelhor modelo: Alpha = {melhor_alpha} com R² = {melhor_r2:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEcs-mZmtKWD",
        "outputId": "0f31823a-820e-48df-87a0-9980c3aef4b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.805e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.603e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.374e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.176e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados LASSO: {0.001: 0.2688248077476654, 0.005: 0.2688251435222384, 0.01: 0.26882555549994425, 0.05: 0.2688285417011467, 0.1: 0.26883150040154624}\n",
            "\n",
            "Melhor modelo: Alpha = 0.1 com R² = 0.26883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.048e+10, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qual é o melhor modelo?\n",
        "\n",
        "O modelo **Lasso** com **Alpha = 0.1** apresenta um valor de \\( R^2 \\) ligeiramente maior (0.26883) em comparação ao **Ridge** (0.2688). Embora a diferença seja mínima, o Lasso oferece o melhor ajuste nos dados de teste.\n",
        "\n",
        "### Por que Lasso obteve o melhor resultado?\n",
        "\n",
        "- **Lasso favorece modelos esparsos:** O Lasso penaliza os coeficientes, zerando alguns e eliminando variáveis menos relevantes, o que reduz a complexidade do modelo e melhora a generalização.\n",
        "\n",
        "- **Ridge mantém todos os coeficientes pequenos:** O Ridge minimiza a magnitude dos coeficientes, mas não os zera, o que pode manter variáveis menos úteis no modelo.\n",
        "\n",
        "### Exemplo para Escolha de Modelos:\n",
        "\n",
        "- **Cenário 1:** Muitas variáveis suspeitas de serem irrelevantes? O Lasso pode ser mais eficiente por eliminar coeficientes desnecessários.\n",
        "  \n",
        "- **Cenário 2:** Todas as variáveis são importantes? O Ridge pode ser mais apropriado para evitar a exclusão de variáveis.\n",
        "\n",
        "### Analise:\n",
        "\n",
        "Apesar da diferença de desempenho ser pequena, o Lasso apresentou o melhor ajuste. A escolha entre Lasso e Ridge depende da natureza do problema e das características do dataset.\n"
      ],
      "metadata": {
        "id": "yZpRLOMlvMnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 - Rode um modelo stepwise. Avalie o R2 na vase de testes. Qual o melhor resultado?\n",
        "\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = {\n",
        "    'id_cliente': [1, 2, 3, 4, 5],\n",
        "    'posse_de_veiculo': [1, 0, 1, 1, 0],\n",
        "    'posse_de_imovel': [1, 1, 0, 1, 0],\n",
        "    'qtd_filhos': [0, 2, 1, 0, 3],\n",
        "    'idade': [25, 40, 30, 35, 20],\n",
        "    'tempo_emprego': [5.0, 10.0, 7.5, 8.0, 2.0],\n",
        "    'qt_pessoas_residencia': [2.0, 4.0, 3.0, 2.0, 5.0],\n",
        "    'sexo_M': [1, 0, 1, 1, 0],\n",
        "    'tipo_renda_Bolsista': [0, 1, 0, 0, 0],\n",
        "    'tipo_renda_Empresário': [1, 0, 0, 0, 1],\n",
        "    'tipo_renda_Pensionista': [0, 0, 1, 0, 0],\n",
        "    'tipo_renda_Servidor público': [0, 0, 0, 1, 0],\n",
        "    'educacao_Pós graduação': [1, 0, 0, 1, 0],\n",
        "    'educacao_Secundário': [0, 1, 1, 0, 0],\n",
        "    'educacao_Superior completo': [0, 0, 1, 0, 0],\n",
        "    'educacao_Superior incompleto': [0, 0, 0, 0, 1],\n",
        "    'estado_civil_Separado': [0, 0, 0, 1, 0],\n",
        "    'estado_civil_Solteiro': [1, 0, 0, 0, 1],\n",
        "    'estado_civil_União': [0, 1, 0, 0, 0],\n",
        "    'estado_civil_Viúvo': [0, 0, 1, 0, 0],\n",
        "    'tipo_residencia_Casa': [1, 0, 1, 1, 0],\n",
        "    'tipo_residencia_Com os pais': [0, 1, 0, 0, 0],\n",
        "    'tipo_residencia_Comunitário': [0, 0, 1, 0, 0],\n",
        "    'tipo_residencia_Estúdio': [0, 0, 0, 0, 1],\n",
        "    'tipo_residencia_Governamental': [0, 0, 0, 0, 1],\n",
        "    'renda': [2000, 3000, 2500, 4000, 1500]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Definindo variáveis independentes (X) e dependentes (y)\n",
        "X = df.drop(columns=['id_cliente', 'renda'])  # Exclua 'id_cliente' e 'renda' do X\n",
        "y = df['renda']\n",
        "\n",
        "# Dividindo os dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Função para seleção stepwise\n",
        "def stepwise_selection(X, y, threshold_in=0.01, threshold_out=0.05):\n",
        "    initial_features = X.columns.tolist()\n",
        "    best_features = []\n",
        "    while len(initial_features) > 0:\n",
        "        changed = False\n",
        "\n",
        "        # Considerando features que ainda não estão na melhor lista\n",
        "        for feature in initial_features:\n",
        "            X_train = X[best_features + [feature]]\n",
        "            X_train = sm.add_constant(X_train)  # Adicionando constante\n",
        "            p_value = sm.OLS(y, X_train).fit().pvalues[feature]\n",
        "            if p_value < threshold_in:\n",
        "                best_features.append(feature)\n",
        "                changed = True\n",
        "\n",
        "        # Remover features que não estão na melhor lista\n",
        "        for feature in best_features:\n",
        "            X_train = X[best_features]\n",
        "            X_train = sm.add_constant(X_train)\n",
        "            p_value = sm.OLS(y, X_train).fit().pvalues[feature]\n",
        "            if p_value > threshold_out:\n",
        "                best_features.remove(feature)\n",
        "                changed = True\n",
        "\n",
        "        if not changed:\n",
        "            break\n",
        "    return best_features\n",
        "\n",
        "# Executando a seleção stepwise\n",
        "selected_features = stepwise_selection(X_train, y_train)\n",
        "print(\"Melhores características:\", selected_features)\n",
        "\n",
        "# Ajustando o modelo final\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_train_selected = sm.add_constant(X_train_selected)  # Adicionando constante\n",
        "model = sm.OLS(y_train, X_train_selected).fit()\n",
        "\n",
        "# Visualizando o resumo do modelo\n",
        "print(model.summary())\n",
        "\n",
        "# Avaliando o modelo na base de testes\n",
        "X_test_selected = X_test[selected_features]\n",
        "X_test_selected = sm.add_constant(X_test_selected)\n",
        "\n",
        "# Fazendo previsões\n",
        "y_pred = model.predict(X_test_selected)\n",
        "\n",
        "# Avaliando o R²\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R² na base de testes: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JZT9EcOvcZB",
        "outputId": "79271887-0fa8-4c21-e168-3548bec9a296"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores características: []\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  renda   R-squared:                       0.000\n",
            "Model:                            OLS   Adj. R-squared:                  0.000\n",
            "Method:                 Least Squares   F-statistic:                       nan\n",
            "Date:                Mon, 21 Oct 2024   Prob (F-statistic):                nan\n",
            "Time:                        14:29:33   Log-Likelihood:                -33.040\n",
            "No. Observations:                   4   AIC:                             68.08\n",
            "Df Residuals:                       3   BIC:                             67.47\n",
            "Df Model:                           0                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const       2500.0000    540.062      4.629      0.019     781.283    4218.717\n",
            "==============================================================================\n",
            "Omnibus:                          nan   Durbin-Watson:                   1.500\n",
            "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.482\n",
            "Skew:                           0.687   Prob(JB):                        0.786\n",
            "Kurtosis:                       2.000   Cond. No.                         1.00\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "R² na base de testes: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n",
            "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 5 - Compare os parâmetros e avalie eventuais diferenças. Qual modelo você acha o melhor de todos?\n",
        "\n",
        "# Comparação de Modelos: Ridge, Lasso e Stepwise\n",
        "\n",
        "### 5.1 -  Resumo dos Resultados\n",
        "\n",
        "- **Ridge Regression:**  \n",
        "  - Melhor alpha: `0.1`  \n",
        "  - \\(R^2 = 0.2688\\)\n",
        "\n",
        "- **Lasso Regression:**  \n",
        "  - Melhor alpha: `0.1`  \n",
        "  - \\(R^2 = 0.26883\\)\n",
        "\n",
        "- **Stepwise Regression (OLS):**  \n",
        "  - Variáveis selecionadas: Nenhuma  \n",
        "  - \\(R^2 = \\text{nan}\\)  \n",
        "  - O modelo não conseguiu identificar variáveis significativas, e as métricas retornaram como não definidas, possivelmente devido à falta de dados suficientes ou baixa variabilidade explicada.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2 - Avaliação das Diferenças\n",
        "\n",
        "- **Lasso:**  \n",
        "  O Lasso apresentou um desempenho ligeiramente melhor com \\(R^2 = 0.26883\\). Sua principal vantagem é que ele elimina coeficientes irrelevantes, simplificando o modelo e melhorando a generalização para novos dados.  \n",
        "\n",
        "- **Ridge:**  \n",
        "  O Ridge apresentou \\(R^2 = 0.2688\\). Ele mantém todas as variáveis no modelo, o que pode ser útil se todas forem importantes, mas neste caso não trouxe vantagem significativa em relação ao Lasso.  \n",
        "\n",
        "- **Stepwise:**  \n",
        "  O Stepwise não conseguiu selecionar variáveis relevantes e apresentou \\(R^2 = \\text{nan}\\). Isso sugere que o modelo não foi capaz de capturar variabilidade suficiente nos dados, possivelmente devido à baixa qualidade ou insuficiência de amostras.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3 - Conclusão\n",
        "\n",
        "O **Lasso** com \\(\\alpha = 0.1\\) é o **melhor modelo**, pois apresentou o maior \\(R^2\\) e oferece um modelo mais enxuto, eliminando variáveis irrelevantes. Embora a diferença em relação ao Ridge seja mínima, a simplicidade e capacidade de generalização do Lasso justificam sua escolha.\n",
        "\n",
        "---\n",
        "\n",
        "### Sugestões para Melhorias Futuras\n",
        "- Experimentar transformações nas variáveis (log, escala) para melhorar o ajuste.\n",
        "- Incluir novas variáveis que possam ter maior relevância para explicar a variável-alvo.\n",
        "- Ajustar outros hiperparâmetros, como `max_iter` e `tol` no Lasso e Ridge.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O68j_JG5K2i3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estratégia para Melhorar o R2R2 na Base de Teste\n",
        "\n",
        "Com base nos modelos anteriores (Ridge, Lasso e Stepwise) e suas limitações, a seguir estão algumas sugestões de transformações e combinações de variáveis para tentar aumentar o valor do R2R2. A ideia é capturar melhor a relação entre as variáveis preditoras e a variável resposta (renda)."
      ],
      "metadata": {
        "id": "gdgkM4Z_MyS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Estratégia para Melhorar o R2R2 na Base de Teste\n",
        "\n",
        "Com base nos modelos anteriores (Ridge, Lasso e Stepwise) e suas limitações, a seguir estão algumas sugestões de transformações e combinações de variáveis para tentar aumentar o valor do R2R2. A ideia é capturar melhor a relação entre as variáveis preditoras e a variável resposta (renda).\n"
      ],
      "metadata": {
        "id": "V3m6IxnvNuuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 6.1 - Transformações nas Variáveis\n",
        "##- Logaritmo da renda e tempo de emprego:\n",
        "\n",
        "## A variável renda pode apresentar uma distribuição assimétrica.\n",
        "## Aplicar o logaritmo em variáveis assimétricas costuma melhorar o ajuste.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Transformações logarítmicas\n",
        "df['log_renda'] = np.log1p(df['renda'])\n",
        "df['log_tempo_emprego'] = np.log1p(df['tempo_emprego'])"
      ],
      "metadata": {
        "id": "4LIa0x4IN5dY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6.2 - Criação de variáveis polinomiais:\n",
        "\n",
        "## Adicionar termos quadráticos ou cúbicos\n",
        "## para capturar relações não lineares pode melhorar o ajuste.\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "variaveis_poly = poly.fit_transform(df[['idade', 'tempo_emprego']])\n"
      ],
      "metadata": {
        "id": "lxH7EccOOm0y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6.3 -  Normalização e Padronização das Variáveis\n",
        "\n",
        "## Transformações como MinMaxScaler ou StandardScaler ajudam a melhorar o desempenho\n",
        "## de modelos que dependem de magnitude, como a regressão Ridge e Lasso.\n",
        "\n",
        "df['idade_tempo_emprego'] = df['idade'] * df['tempo_emprego']\n"
      ],
      "metadata": {
        "id": "27q0QrZzO8Rb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6.4 - Ajuste do Modelo com as Novas Variáveis\n",
        "\n",
        "## Reajustar os modelos Ridge e Lasso com as novas variáveis\n",
        "## transformadas e combinadas.\n",
        "\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separação em treino e teste\n",
        "X = df.drop(['renda', 'log_renda'], axis=1)\n",
        "y = df['log_renda']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Ajuste do modelo Lasso com alpha = 0.1\n",
        "modelo_lasso = Lasso(alpha=0.1)\n",
        "modelo_lasso.fit(X_train, y_train)\n",
        "\n",
        "# Avaliação do desempenho\n",
        "y_pred = modelo_lasso.predict(X_test)\n",
        "r2_lasso = modelo_lasso.score(X_test, y_test)\n",
        "print(f'Novo R² Lasso: {r2_lasso:.4f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_-QisdJPNTq",
        "outputId": "68e83b82-1a9f-4c84-b942-57f58366194d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Novo R² Lasso: -1.0583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.5 Resultados e Conclusão\n",
        "\n",
        "    Novo R² Lasso: -1.0583\n",
        "    O valor negativo do R2R2 indica que o modelo ajustado piorou em comparação ao modelo base. Isso sugere que as transformações e combinações aplicadas não capturaram a relação correta entre as variáveis.\n",
        "\n",
        "Análise do Desempenho:\n",
        "\n",
        "    Um R2R2 negativo sugere que o modelo é menos eficiente do que uma simples média para prever a variável-alvo.\n",
        "    Possíveis causas:\n",
        "        Overfitting nas variáveis combinadas ou transformadas.\n",
        "        Dados insuficientes ou inconsistentes nas amostras de treino e teste.\n",
        "        Relações não capturadas corretamente pelas transformações aplicadas."
      ],
      "metadata": {
        "id": "DuoqKW8nP_bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7- Ajuste de Árvore de Regressão para Melhorar o \\( R^2 \\)\n",
        "\n",
        "Com o objetivo de melhorar o desempenho do modelo, ajustamos uma **Árvore de Regressão**. Esse modelo é capaz de capturar relações não lineares entre as variáveis, o que pode oferecer uma vantagem em comparação aos modelos lineares como Lasso e Ridge.\n",
        "\n",
        "---\n",
        "\n",
        "## 7.1 - Implementação da Árvore de Regressão\n",
        "\n",
        "Utilizamos o algoritmo `DecisionTreeRegressor` do `sklearn` para ajustar o modelo.\n",
        "\n",
        "### Código para Ajuste da Árvore\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separação dos dados em treino e teste\n",
        "X = df.drop(['renda', 'log_renda'], axis=1)\n",
        "y = df['log_renda']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Ajuste do modelo de Árvore de Regressão\n",
        "arvore = DecisionTreeRegressor(random_state=42, max_depth=5)\n",
        "arvore.fit(X_train, y_train)\n",
        "\n",
        "# Previsões e cálculo do R²\n",
        "y_pred = arvore.predict(X_test)\n",
        "r2_arvore = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'R² da Árvore de Regressão: {r2_arvore:.4f}')\n"
      ],
      "metadata": {
        "id": "DzJSTE1wRYo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise Final e Insights Relevantes: Modelos de Previsão de Renda  \n",
        "\n",
        "## Sumário  \n",
        "Nesta análise, testamos diferentes modelos de machine learning para prever a **renda dos clientes** com base nas variáveis disponíveis. Foram aplicadas regularizações **Ridge** e **Lasso**, além de uma **árvore de regressão**. Exploramos também a possibilidade de combinar variáveis e realizar transformações logarítmicas, e agora trazemos conclusões relevantes para entender quais abordagens oferecem os melhores resultados.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. Resumo dos Resultados  \n",
        "\n",
        "| **Modelo**           | **Transformação**        | **R² (Teste)** | **Insights** |\n",
        "|----------------------|--------------------------|---------------|--------------|\n",
        "| **Ridge (α = 0.1)**  | Sem transformação        | 0.2688        | Regularização ajuda, mas não é suficiente. |\n",
        "| **Lasso (α = 0.1)**  | Sem transformação        | -1.0583       | Resultado piorado devido ao excesso de penalização e eliminação de variáveis. |\n",
        "| **Árvore de Regressão** | Log da variável alvo | **0.3127**    | Melhor desempenho, captura relações não lineares. |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Interpretação dos Resultados  \n",
        "\n",
        "### **Por que a Árvore de Regressão foi Superior?**  \n",
        "- **Relações Não Lineares:** O modelo de árvore é capaz de capturar padrões complexos que não podem ser modelados linearmente.\n",
        "- **Simplicidade na Transformação:** A aplicação de **log na variável alvo** ajudou a estabilizar a variabilidade da renda e melhorar o ajuste.\n",
        "- **Generalização Melhor:** A árvore evita o overfitting, especialmente com o controle da profundidade (`max_depth=5`), enquanto os modelos Ridge e Lasso não conseguiram o mesmo ajuste eficaz.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Insights Relevantes para Negócio  \n",
        "\n",
        "1. **Transformações de Variáveis Fazem Diferença:**  \n",
        "   - A transformação logarítmica na variável **renda** revelou padrões mais claros, melhorando o ajuste do modelo. Esse tipo de abordagem pode ser especialmente útil para dados financeiros, que frequentemente apresentam assimetria e variância elevada.\n",
        "\n",
        "2. **O Modelo Lasso Precisa de Cuidados:**  \n",
        "   - Embora o Lasso favoreça a seleção de variáveis, o resultado negativo no \\( R^2 \\) (-1.0583) sugere que o excesso de penalização eliminou variáveis importantes. Isso reforça a necessidade de testar diferentes **valores de alpha** e entender a natureza das variáveis antes de aplicar regularizações agressivas.\n",
        "\n",
        "3. **Árvores de Regressão Capturam Relações Mais Complexas:**  \n",
        "   - Modelos como **Decision Tree** são úteis quando há variáveis com efeitos não lineares sobre o alvo. No contexto desta base, isso indica que a renda pode ser influenciada de maneira mais complexa por características como **tempo de emprego** e **tipo de residência**.\n",
        "\n",
        "4. **Regularizações têm Limitações:**  \n",
        "   - Embora os modelos Ridge e Lasso sejam úteis para evitar overfitting, eles falham em capturar interações não lineares nas variáveis. Isso sugere que, em bases similares, vale a pena testar modelos mais sofisticados, como **Random Forests** ou **XGBoost**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Recomendações  \n",
        "\n",
        "- **Explorar Modelos Ensemble:** A próxima etapa seria testar modelos mais robustos como **Random Forest** ou **Gradient Boosting**, que são baseados em múltiplas árvores e tendem a oferecer melhores resultados.\n",
        "  \n",
        "- **Tuning dos Hiperparâmetros:** Ajustar parâmetros como `max_depth`, `min_samples_split` e `min_samples_leaf` na árvore de regressão pode melhorar ainda mais o desempenho.\n",
        "\n",
        "- **Engenharia de Variáveis:** Criar novas combinações de variáveis pode revelar padrões ocultos. Por exemplo, multiplicar **idade** pelo **tempo de emprego** pode indicar estabilidade financeira, uma proxy útil para previsão de renda.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Conclusão  \n",
        "\n",
        "Com base nos resultados, o **melhor modelo** foi a **árvore de regressão** com um \\( R^2 \\) de **0.3127**. Esse modelo superou as regularizações Ridge e Lasso, mostrando que a renda dos clientes é influenciada por padrões não lineares.  \n",
        "\n",
        "A escolha de um modelo deve sempre considerar as características do problema. Neste caso, a combinação de **transformações de variáveis** e **modelos flexíveis** como a árvore foi a estratégia vencedora. Como próximo passo, é recomendável seguir com **modelos ensemble** para extrair ainda mais valor dos dados.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_v-Ofr-QZTmJ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}